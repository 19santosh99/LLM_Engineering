{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "A comparative analysis is needed to evaluate the performance and capabilities of ChatGPT 4.0 and Claude 3.5 Sonnet in converting Python code to optimized C++ code.\n",
    "\n",
    "### Objectives\n",
    " - Convert existing Python codebase to C++\n",
    " - Optimize the converted code for better performance\n",
    " - Compare the effectiveness of AI assistants in code conversion and optimization\n",
    "### Key Requirements\n",
    "- Code Conversion Requirements\n",
    "- Maintain functional equivalence with the original Python code\n",
    " - Properly handle memory management in C++\n",
    "- Ensure type safety and appropriate C++ data structures\n",
    "- Preserve code readability and maintainability\n",
    "- Optimization Requirements\n",
    "- Improve execution speed\n",
    "- Reduce memory usage\n",
    "- Implement C++-specific optimizations\n",
    "- Utilize modern C++ features (C++11 and above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system promt and user prompts\n",
    "system_prompt = \"You are an expert C++ software engineer specializing in system programming, optimization, and cross-language code conversion. Your task is to help convert and optimize a Python environment file loading system to C++, following C++ standards and best practices.\"\n",
    "system_prompt += 'do not put commas or any character for the numbers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(python):\n",
    "    user_prompt = \"Convert the following Python code to C++ code, following best practices, make sure the outputs of python code and c++ code should be accurate.\"\n",
    "    user_prompt += \"\\n\\nHere is the Python code:\\n\\n\"\n",
    "    user_prompt += python\n",
    "    user_prompt += \"\\n\\nNow, convert the Python code to C++ code, ensuring that the C++ code maintains the same functionality and produces the same output as the Python code. Make sure to handle memory management properly and use appropriate C++ data structures. Optimize the C++ code for better performance, considering factors such as execution speed and memory usage. check the syntax of the code before response\"\n",
    "    user_prompt += \"\\n\\nPlease provide the optimized C++ code as the output. Do not give any text or explimation before and after c++ code. Give in such a way that the output is saved as a file and I should be able to run it without modifying\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = \"\"\"\n",
    "import time\n",
    "\n",
    "def calculate(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\n",
    "start_time = time.time()\n",
    "result = calculate(100_000_000, 4, 1) * 4\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Result: {result:.12f}\")\n",
    "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt_for(pi)}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def stream_openai_response(model: str = \"gpt-4\"):\n",
    "    \"\"\"\n",
    "    Stream responses from OpenAI API\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dictionaries\n",
    "        model: OpenAI model to use\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize OpenAI client\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        \n",
    "        # Create streaming response\n",
    "        stream = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        \n",
    "        # Process the stream\n",
    "        full_response = \"\"\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                content = chunk.choices[0].delta.content\n",
    "                # Clean the content by removing markdown code markers\n",
    "                cleaned_content = content.replace('```cpp', '').replace('```', '')\n",
    "                cleaned_content = cleaned_content.replace('cpp', '')\n",
    "                print(cleaned_content, end='', flush=True)\n",
    "                full_response += cleaned_content\n",
    "                \n",
    "        return full_response\n",
    "                \n",
    "    except openai.APIError as e:\n",
    "        print(f\"OpenAI API Error: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\", file=sys.stderr)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_openai_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "claude_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "def stream_claude_response(model: str = \"claude-3-5-sonnet-20240620\"):\n",
    "    try:\n",
    "        result = claude_client.messages.stream(\n",
    "            model=model,\n",
    "            max_tokens=2000,\n",
    "            system=system_prompt,\n",
    "            messages=[{\"role\": \"user\", \"content\": user_prompt_for(pi)}],\n",
    "        )\n",
    "\n",
    "        reply = \"\"\n",
    "        with result as stream:\n",
    "            for text in stream.text_stream:\n",
    "                reply += text\n",
    "                print(text, end=\"\", flush=True)  # This will show the streaming response\n",
    "        return reply\n",
    "\n",
    "    except anthropic.APIError as e:\n",
    "        print(f\"Claude API Error: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\", file=sys.stderr)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_claude_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(cpp, filename):\n",
    "    code = cpp.replace(\"```cpp\",\"\").replace(\"```\",\"\")\n",
    "    with open(f\"{filename}.cpp\", \"w\") as f:\n",
    "        f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_output(stream_claude_response(), \"optimisedcode.cpp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelSelection(model_choice):\n",
    "    if model_choice == \"Claude\":\n",
    "        return stream_claude_response()\n",
    "    if model_choice == \"GPT\":\n",
    "        return stream_openai_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def create_gradio_interface():\n",
    "    with gr.Blocks() as app:\n",
    "        with gr.Row():\n",
    "            python_input = gr.Code(\n",
    "                label=\"Python Code\",\n",
    "                language=\"python\",\n",
    "                value=pi,\n",
    "                elem_classes=[\"code-box\"]\n",
    "            )\n",
    "            cpp_output = gr.Code(\n",
    "                label=\"C++ Code\",\n",
    "                language=\"cpp\",\n",
    "                interactive=True,\n",
    "                elem_classes=[\"code-box\"]\n",
    "            )\n",
    "        \n",
    "        with gr.Row():\n",
    "            model_choice = gr.Radio(\n",
    "                choices=[\"Claude\", \"GPT\"],\n",
    "                value=\"Claude\",\n",
    "                label=\"Choose AI Model\"\n",
    "            )\n",
    "        \n",
    "        with gr.Row():\n",
    "            translate_button = gr.Button(\"Translate\")\n",
    "        \n",
    "        translate_button.click(\n",
    "            fn=modelSelection,\n",
    "            inputs=[model_choice],\n",
    "            outputs=cpp_output\n",
    "        )\n",
    "    \n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = create_gradio_interface()\n",
    "app.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When analyzing the execution speed differences between Python and the AI-generated C++ code, we observed significant performance improvements. The C++ code produced by Claude demonstrated exceptional efficiency, running 60,000 times faster than its Python counterpart. Similarly, the C++ code generated by GPT also showed remarkable performance, executing 40,000 times faster than the original Python implementation. This stark contrast in runtime performance highlights the substantial speed advantages of the AI-translated C++ code over the original Python version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Version 1(Current): The existing Gradio interface features a dual-panel display system: one side shows the original Python code, while the other presents its AI-translated version. Users can toggle between Claude or GPT as their preferred translation model.\n",
    "\n",
    "- Version 2(Next Steps): Future development will focus on embedding C++ compilation capabilities directly within the Gradio interface, allowing users to instantly compare performance metrics between the Python and C++ implementations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
