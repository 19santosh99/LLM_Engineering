{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKs1PM-O-VQa"
      },
      "source": [
        "# Models\n",
        "\n",
        "Looking at the lower level API of Transformers - the models that wrap PyTorch code for the transformers themselves.\n",
        "\n",
        "This notebook can run on a low-cost or free T4 runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NthhKJRwX1iS",
        "outputId": "04c93a97-8086-4052-a466-b0c22702f193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9zvDGWD5pKp"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyKWKWSw7Iqp"
      },
      "source": [
        "# Sign in to Hugging Face\n",
        "\n",
        "1. If you haven't already done so, create a free HuggingFace account at https://huggingface.co and navigate to Settings, then Create a new API token, giving yourself write permissions\n",
        "\n",
        "2. Press the \"key\" icon on the side panel to the left, and add a new secret:\n",
        "`HF_TOKEN = your_token`\n",
        "\n",
        "3. Execute the cell below to log in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd7cEDUC6Lkq",
        "outputId": "c939115e-3659-4980-9462-a907b4c3056b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtN7OKILQato"
      },
      "outputs": [],
      "source": [
        "# instruct models\n",
        "\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "PHI3 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "GEMMA2 = \"google/gemma-2-2b-it\"\n",
        "QWEN2 = \"Qwen/Qwen2-7B-Instruct\" # exercise for you\n",
        "MIXTRAL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" # If this doesn't fit it your GPU memory, try others from the hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgxCLBJIT5Hx"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "  ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSiYqPn87msu"
      },
      "source": [
        "# Accessing Llama 3.1 from Meta\n",
        "\n",
        "In order to use the fantastic Llama 3.1, Meta does require you to sign their terms of service.\n",
        "\n",
        "Visit their model instructions page in Hugging Face:\n",
        "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n",
        "\n",
        "At the top of the page are instructions on how to agree to their terms. If possible, you should use the same email as your huggingface account.\n",
        "\n",
        "In my experience approval comes in a couple of minutes. Once you've been approved for any 3.1 model, it applies to the whole family of models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhOgL1p_T6-b"
      },
      "outputs": [],
      "source": [
        "# Quantization Config - this allows us to load the model into memory and use less memory\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi8YXiwJHF59"
      },
      "outputs": [],
      "source": [
        "# Tokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5jly421tno3"
      },
      "outputs": [],
      "source": [
        "# The model\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdbYaT8hWXWE"
      },
      "outputs": [],
      "source": [
        "memory = model.get_memory_footprint() / 1e6\n",
        "print(f\"Memory footprint: {memory:,.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0qmAD5ZtqWA"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkYEXzbotcud"
      },
      "outputs": [],
      "source": [
        "outputs = model.generate(inputs, max_new_tokens=80)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oL0RWU2ttZf"
      },
      "outputs": [],
      "source": [
        "# Clean up\n",
        "\n",
        "del inputs, outputs, model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO_VYZ3DZ7cs"
      },
      "outputs": [],
      "source": [
        "# Wrapping everything in a function - and adding Streaming\n",
        "\n",
        "def generate(model_name, messages, max_tokens=80):\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    try:\n",
        "        # Prepare inputs\n",
        "        inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "        \n",
        "        # Initialize streamer and model\n",
        "        streamer = TextStreamer(tokenizer)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name, \n",
        "            device_map=\"auto\", \n",
        "            quantization_config=quant_config,\n",
        "            torch_dtype=torch.bfloat16  # Add explicit dtype\n",
        "        )\n",
        "        \n",
        "        # Generate with error handling\n",
        "        outputs = model.generate(\n",
        "            inputs, \n",
        "            max_new_tokens=max_tokens, \n",
        "            streamer=streamer,\n",
        "            pad_token_id=tokenizer.pad_token_id,  # Add explicit padding token\n",
        "            do_sample=True  # Enable sampling for more creative responses\n",
        "        )\n",
        "    finally:\n",
        "        # Ensure cleanup happens even if an error occurs\n",
        "        cleanup_vars = [tokenizer, streamer, model, inputs]\n",
        "        for var in cleanup_vars:\n",
        "            if var in locals():\n",
        "                del var\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFjaY4Pdvbfy"
      },
      "outputs": [],
      "source": [
        "generate(PHI3, messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxZQmZDCe4Jf"
      },
      "source": [
        "## Accessing Gemma from Google\n",
        "\n",
        "A student let me know (thank you, Alex K!) that Google also now requires you to accept their terms in HuggingFace before you use Gemma.\n",
        "\n",
        "Please visit their model page at this link and confirm you're OK with their terms, so that you're granted access.\n",
        "\n",
        "https://huggingface.co/google/gemma-2-2b-it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1JW41D-viGy"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "  ]\n",
        "generate(GEMMA2, messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m8yjMB3ZTp3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
