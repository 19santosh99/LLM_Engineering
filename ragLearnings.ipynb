{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert Knowledge Worker\n",
    "\n",
    "#### A question answering agent that is an expert knowledge worker. To be used by employees of Insurellm, an Insurance Tech company. The agent needs to be accurate and the solution should be low cost.\n",
    "\n",
    "This project will use RAG (Retrieval Augmented Generation) to ensure our question/answering assistant has high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import MarkdownTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from typing import List\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "model_name = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"knowledge-base/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_document_structure(root_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Analyze and print the structure of markdown files in the directory\n",
    "    \n",
    "    Args:\n",
    "        root_dir: Root directory to analyze\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            level = root.replace(root_dir, '').count(os.sep)\n",
    "            indent = ' ' * 4 * level\n",
    "            print(f\"{indent}{os.path.basename(root)}/\")\n",
    "            subindent = ' ' * 4 * (level + 1)\n",
    "            for f in files:\n",
    "                if f.endswith('.md'):\n",
    "                    print(f\"{subindent}{f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing directory structure: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyzing directory structure...\")\n",
    "analyze_document_structure(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_tokenize_markdown_files(root_dir: str) -> List:\n",
    "    \"\"\"\n",
    "    Load and tokenize markdown files from a nested directory structure\n",
    "    \n",
    "    Args:\n",
    "        root_dir: Root directory containing markdown files\n",
    "    \n",
    "    Returns:\n",
    "        List of processed documents with tokens\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the directory loader for markdown files\n",
    "        loader = DirectoryLoader(\n",
    "            root_dir,\n",
    "            glob=\"**/*.md\",  # Recursively match all .md files\n",
    "            loader_cls=UnstructuredMarkdownLoader,\n",
    "            show_progress=True\n",
    "        )\n",
    "        \n",
    "        # Load all documents\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Initialize the markdown text splitter\n",
    "        markdown_splitter = MarkdownTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        \n",
    "        # Split documents into chunks\n",
    "        chunks = markdown_splitter.split_documents(documents)\n",
    "            \n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing markdown files: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing markdown files and getting chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 31/32 [00:00<00:00, 83.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process documents\n",
    "print(\"\\nProcessing markdown files and getting chunks...\")\n",
    "chunks = load_and_tokenize_markdown_files(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_statistics(documents: List) -> dict:\n",
    "    \"\"\"\n",
    "    Get statistics about the processed documents\n",
    "    \n",
    "    Args:\n",
    "        documents: List of processed documents\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing document statistics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stats = {\n",
    "            'total_documents': len(documents),\n",
    "            'total_tokens': sum(len(doc.page_content.split()) for doc in documents),\n",
    "            'average_chunk_size': sum(len(doc.page_content) for doc in documents) / len(documents)\n",
    "        }\n",
    "        return stats\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating document statistics: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document Statistics:\n",
      "Total Documents: 112\n",
      "Total Tokens: 12598\n",
      "Average Chunk Size: 820.55 characters\n"
     ]
    }
   ],
   "source": [
    "stats = get_document_statistics(chunks)\n",
    "print(\"\\nDocument Statistics:\")\n",
    "print(f\"Total Documents: {stats['total_documents']}\")\n",
    "print(f\"Total Tokens: {stats['total_tokens']}\")\n",
    "print(f\"Average Chunk Size: {stats['average_chunk_size']:.2f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 112 documents\n"
     ]
    }
   ],
   "source": [
    "# lets setup openAIEmbeddings\n",
    "\n",
    "# Initialize OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(api_key=openai_api_key)\n",
    "\n",
    "# Chroma vector store\n",
    "# Initialize ChromaClient and create a vector store\n",
    "db_name = \"vector_db\"\n",
    "\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 1,536 dimensions\n"
     ]
    }
   ],
   "source": [
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=model_name)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insurellm is an innovative insurance tech firm founded in 2015 by Avery Lancaster. The company has grown to 200 employees and operates 12 offices across the US by 2024. Insurellm offers four insurance software products: Carllm (for auto insurance companies), Homellm (for home insurance companies), Rellm (an enterprise platform for the reinsurance sector), and Marketllm (a marketplace connecting consumers with insurance providers). The firm serves over 300 clients worldwide and provides services such as regulatory compliance tools, client and broker portals, and 24/7 technical support.\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you describe Insurellm in a few sentences\"\n",
    "result = conversation_chain.invoke({\"question\":query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insurellm offers products related to the following types of insurance:\n",
      "\n",
      "1. Auto Insurance (through Carllm, a portal for auto insurance companies)\n",
      "2. Home Insurance (through Homellm, a portal for home insurance companies)\n",
      "3. Reinsurance (through Rellm, an enterprise platform for the reinsurance sector)\n",
      "4. Marketplace services for connecting consumers with insurance providers (through Marketllm) \n",
      "\n",
      "However, specific insurance policies or plans are not detailed in the provided context.\n"
     ]
    }
   ],
   "source": [
    "query = \"what are the types of insurances available in Insurellm in few words\"\n",
    "result = conversation_chain.invoke({\"question\":query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for RAG\n",
    "1. get access to all the files in the folder\n",
    "2. Use the DirectoryLoader to read all the files in the folder and create documents\n",
    "3. use these documents from DirectoryLoader and split into chunks using the MarkdownTextSplitter or any kinds of splitter\n",
    "4. So once we have the chunks we have to create a vector store, can use Chroma or Fiassor pinecone.\n",
    "5. for the vector store we use openAI Embeddings\n",
    "6. Next use the langchain abstraction to create an openai client\n",
    "7. similarly create a memory(ConversationBufferMemory) for the chat\n",
    "8. Now time to put all the vectors, memory and llm into the conversation_chain(ConversationalRetrievalChain)\n",
    "9. query the  to see the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
